# Human-aligned Machine Intelligence (HAMI)
### Technical University of Munich

This repository contains all our experiments and models focused on AI-safety 
and AI-alignment. For the official website visit:
https://sites.google.com/view/ai-safety-tum/home

The implementations include (will get updated with each new experiment):

1. Proximal Policy Optimization (PPO)

#### Usage
The scripts use `Fire`, which makes them easy to train and test.
To train and test an agent, run the script with the CLI argument `train` and `test`
respectively. After training, the model will be stored in the `models` directory.